

\beginedxvertical{Page One}

\beginedxtext{Preliminaries}





At the end of this sequence, and after some practice, you should be able to:

\begin{itemize}
\item Recognize properties of orthogonal matrices. 
\end{itemize}


For time budgeting purposes, this sequence has 3 videos totaling 17 minutes, 
plus some questions.  




\endedxtext

\endedxvertical



\beginedxvertical{Transposes and Inner Products}

\beginedxproblem{Warm-ups 1}{\dpa3}


Let $v = \left[ \begin{array}{c} 3 \\ 4 \\ 7 \end{array} \right]$, and let $w  = \left[ \begin{array}{cc} 100  \\ 10  \\ 1  \end{array} \right]$.  


What is the dot product of $v$ with $w$?  


\edXabox{type="numerical" expect="347" feqin="1"  tolerance=".01"}

\edXsolution{
Just calculate: $(3)(100)+(4)(10) + (7)(1) = 347$.    
}

\endedxproblem


\beginedxproblem{Warm-ups 2}{\dpa3}

Using the same $v$ and $w$ as above,  treat $v$ and $w$ as  $3\times 1$ matrices.  What is the matrix $v^t$?   

\input{matrixentry.tex}


\edXabox{type="custom" cfn="MatrixEntry" expect="[[3,4,7]]"}


What is the matrix product $v^t w$?  

\edXabox{type="custom" cfn="MatrixEntry" expect="[[347]]"}


\edXsolution{
Since $v$ is a $3\times 1$ matrix, its transpose will be a $1\times 3$ matrix, specifically
$\left[ \begin{array}{ccc} 3 & 4 & 7 \end{array} \right]$.  

We will get the  $1\times 1$ matrix $\left[ \begin{array}{c} 347  \end{array} \right]$  when we multiply by $w$.  

}



\endedxproblem


\doedxvideo{Transposes and Inverses}{hu_z5ibA130}


\endedxvertical



\beginedxvertical{Playing with Transposes}



\beginedxproblem{True or False}{\dpa1}

If $v$ and $w$ are vectors in $\R^n$  and $\langle v, w\rangle  = -2$, then 
the matrix product $vw^t$ is the matrix $\left[ \begin{array}{c} -2  \end{array} \right]$.  




\edXabox{expect="False" options="True","False"}

\edXsolution{
False.  $v$ is a $n \times 1$ matrix, and $w^t$ is a $1\times n$ matrix, so $vw^t$ is an $n\times n$
matrix.  To get $\left[ \begin{array}{c} -2  \end{array} \right]$, we would need to take
$w^tv$ or $v^tw$ (both would give the same result; note that $\langle v,w\rangle = \langle w, v\rangle$).  

}



\endedxproblem



\beginedxproblem{Orthogonal Columns}{\dpa1}

Suppose the columns of $A$ form an orthogonal but not necessarily orthonormal basis of $\R^n$.  What
could we say about $A^t A$?  



\edXabox{type="multichoice" expect="It is a diagonal matrix, but not necessarily a
scalar multiple of the identity matrix" options="It is the identity matrix","It is a multiple of the identity matrix, but not necessarily the identity","It is a diagonal matrix, but not necessarily a
scalar multiple of the identity matrix","None of the above"}

\edXsolution{

The entry in the $ij$ spot of $A^tA$ will be the dot product of the $i$th column of $A$ with the $j$th column
of $A$.  This will be zero when $i\ne j$ (i.e., when the entry is off the main diagonal) 
so the matrix will be diagonal.  The entry in the $ii$ spot will be 
$\langle v_i, v_i\rangle = \|v_i\|^2$, which could be different for the various columns.  So the entries
along the diagonal are not necessarily the same.  
}



\endedxproblem

\endedxvertical



\beginedxvertical{Preserving Inner Products and Norms}


\doedxvideo{Preserving Inner Products and Norms}{AwKwv0S5s2o}


\beginedxproblem{True or False 2}{\dpa1}

If the columns of $A$ form an orthonormal basis of $\R^n$, and $\{w_1; w_2;\ldots w_k\}$ is an
orthogonal list of vectors in $\R^n$, then $\{Aw_1; Aw_2;\ldots Aw_k\}$
is also an orthogonal list of vectors.  


\edXabox{expect="True" options="True","False"}

\edXsolution{
From the video, we can conclude that $A$ preserves inner products.  Thus, if $\langle w_i, w_j \rangle = 0$ for $i\ne j$, then we also have
$\langle Aw_i, Aw_j \rangle = 0$ for $i\ne j$.
}



\endedxproblem

\beginedxproblem{True or False 3}{\dpa1}

If the columns of $A$ form an orthonormal basis of $\R^n$, and $\{w_1; w_2;\ldots w_k\}$ is an
orthonormal list of vectors in $\R^n$, then $\{Aw_1; Aw_2;\ldots Aw_k\}$
is also an orthonormal list of vectors.  


\edXabox{expect="True" options="True","False"}

\edXsolution{
From the video, we can conclude that $A$ preserves norms.  Therefore, since $\|w_i\| = 1$ for all $i$,
we also have $\|Aw_i\| = 1$ for all $i$.  Hence the list is orthonormal, not just orthogonal.  
}



\endedxproblem


\beginedxproblem{True or False 4}{\dpa1}

If the columns of $A$ form an orthonormal basis of $\R^n$, then $\langle Av, w\rangle  = \langle v, Aw \rangle$
for all $v,w\in \R^n$.  


\edXabox{expect="False" options="True","False"}

\edXsolution{
That is not what we mean when we say $A$ preserves inner products!  Rather, we mean that
$\langle Av,Aw\rangle = \langle v,w\rangle$ for all $v,w\in \R^n$. 
}



\endedxproblem



\endedxvertical


\beginedxvertical{Orthogonal Matrices}


\doedxvideo{Orthogonal Matrices}{6zVVf0rzOdQ}



\beginedxtext{Orthogonal Matrix Definition}

{\keya{\bf{Theorem/Definition}}}.  

Given an $n\times n$ real matrix $A$, the following four conditions are all equivalent.

\begin{itemize}
\item
The columns of $A$ form an orthonormal basis of $\R^n$.  
\item
$A^t = A\inv$.  
\item
$A$ preserves inner products; i.e., $\langle Av,Aw\rangle = \langle v,w\rangle$ for all $v,w\in \R^n$.
\item
$A$ preserves norms; i.e., $\| Av\| = \|v\|$ for all $v\in \R^n$.
\end{itemize}

If any/all of them are true, we say that $A$ is an {\keyb{\bf{orthogonal
matrix}}}.  

\endedxtext


\beginedxproblem{True or False 5}{\dpa1}

Let $T: \R^3 \rightarrow \R^3$ be a reflection across some plane through the origin, and let $A$ be its
standard matrix.  Then the columns of $A$ must form an orthonormal basis of $\R^3$.  


\edXabox{expect="True" options="True","False"}

\edXsolution{
We can see that $T$ preserves distances and thus norms, so $A$ does as well.  Hence $A$ is an
orthogonal matrix and its columns form an orthonormal basis of $\R^3$.   
}



\endedxproblem


\endedxvertical





