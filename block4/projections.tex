

\beginedxvertical{Page One}

\beginedxtext{Preliminaries}





At the end of this sequence, and after some practice, you should be able to:

\begin{itemize}
\item Recognize the properties of orthogonal projections.  
\item Compute an orthogonal projection. 
\item Use the Gram-Schmidt algorithm to produce an orthogonal basis.  
\end{itemize}


For time budgeting purposes, this sequence has 5 videos totaling 40 minutes, 
plus some questions.  




\endedxtext

\endedxvertical



\beginedxvertical{Finding Closest Vectors}

\beginedxproblem{Closest Vector}{\dpa3}

Let $W$ be the subspace of $\R^3$ given by 
\[ W = \left\{ \left[  \begin{array}{c} a_1 \\ a_2 \\ 0 \end{array} \right] \ : \ a_1, a_2 \in \R \right\}.\]
(In other words, you can think of $W$ as the $xy$ plane inside $\R^3$.  

Let $v = \left[ \begin{array}{c} 3 \\ 4 \\ 7 \end{array} \right].$  What is the vector $w\in W$ that 
is closest to $v$ (i.e., the vector such that the distance from $w$ to $v$ is smallest)?  


\begin{center}
\includesvg[300]{c4s2projection}   
\end{center}


\input{vectorentry.tex}


\edXabox{type="custom" cfn="VectorEntry" expect="[[3],[4],[0]]"}

\edXsolution{
If $w = \left[ \begin{array}{c} a_1 \\ a_2 \\ 0 \end{array} \right]$, then the
distance from $v$ to $w$ is 
\[ \|w-v\| = \sqrt{(a_1-3)^2 + (a_2-4)^2 + 7^2}. \]  
At minimum, this distance is $\sqrt{49} = 7$, and it equals 7 exactly when $a_1 = 3$ and $a_2=4$.  
In other words, the distance is minimized when 
$w =  \begin{array}{c} 3 \\ 4 \\ 0  \end{array} \right].$
}

\endedxproblem



\doedxvideo{Introducing the Orthogonal Projection}{R1XIoRDaAB8}




\beginedxproblem{Exploring the formula 1}{\dpa1}

The formula given in the video is $\ds \sum_{i=1}^k \langle v, w_i\rangle w_i.$  Let's examine this
a little more closely.

For each $i$, what kind of object is $\langle v, w_i \rangle$?  

\edXabox{expect="a scalar" options="a scalar","a vector in V","a matrix","none of these"}

\edXsolution{
The inner product of two vectors must be a scalar.  
}

\endedxproblem


\beginedxproblem{Exploring the formula 2}{\dpa1}

 

\edXinline{This means that  $\ds \sum_{i=1}^k \langle v, w_i\rangle w_i$  is}
\edXabox{expect="a linear combination of" options="an inner product of","a scalar multiple of","a linear combination of","a linear transformation of","a matrix whose columns are" inline="1"} \edXabox{expect="the collection of vectors w_1...w_k" options="v","one of the vectors w_i","the collection of vectors w_1...w_k" inline="1"}

\edXinline{Therefore, it is}
\edXabox{expect="a vector in W" options="a scalar","a vector in W","a vector in V but not necessarily in W","a matrix","none of these"  inline="1"}


\edXsolution{
Each $\langle v, w_i\rangle w_i$ is a scalar times a vector $w_i$.  When we take the sum of these,
it is (by definition) a linear combination of $\{w_1; w_2; \ldots w_k\}$, and thus is a vector.  Specifically,
it is a vector in $W$, since all the $w_i$ are in $W$.      
}

\endedxproblem



\endedxvertical



\beginedxvertical{The Projection Formula Proof}


\doedxvideo{Proving the Orthogonal Projection Formula}{V5WPw2C-ixY}



\beginedxproblem{Using the Projection Formula?}{\dpa1}

Let $W$ be the span of $\{w_1; w_2\}$, where 
$w_1 = \left[ \begin{array}{c} 1 \\ 2 \\ 3 \end{array} \right]$
and 
$w_2 = \left[  \begin{array}{c} 3 \\ -3 \\ 1 \end{array} \right]$.
Then, for any $v\in \R^3$, 
\[\mathrm{Proj}_W(v) = \langle v, w_1 \rangle w_1 + \langle v, w_2 \rangle w_2.\]

\edXabox{expect="False" options="True","False"}

\edXsolution{
The formula given in the video does not work if $\{w_1; w_2\}$ is not an orthonormal basis.  
}

\endedxproblem


\endedxvertical


\beginedxvertical{Full Generality}


\doedxvideo{Final Projection Formula}{DzwlyaY4LmY}



% \beginedxproblem{Using the Projection Formula? 2}{\dpa1}

% Let $W$ be the span of $\{w_1; w_2\}$, where 
% $w_1 = \begin{array}{c} 1 \\ 2 \\ 3 \end{array} \right]$
% and 
% $w_2 = \begin{array}{c} 3 \\ -3 \\ 1 \end{array} \right]$.
% Then, for any $v\in \R^3$, the closest vector in $W$ to $v$ is given by the formula 
% \[\frac{\langle v, w_1 \rangle}{\langle w_1, w_1 \rangle} w_1 + \frac{\langle v, w_2 \rangle}{\langle w_2, w_2 \rangle} w_2.\]

% \edXabox{expect="True" options="True","False"}

% \edXsolution{
% This is the projection formula given in the video.  Since $\{w_1; w_2\}$ is an orthogonal basis of $W$,
% this 
% }

% \endedxproblem

\beginedxtext{The Orthogonal Projection}

If $V$ is a vector space with an inner product, and $W$ is a subspace of $V$ with an orthogonal basis 
$\{u_1; u_2; \ldots u_k\}$, then the {\keyb{\bf{orthogonal
projection}}} of a vector $v\in V$ onto $W$ is given by the formula
\[\mathrm{Proj}_W(v) =  \sum_{i=1}^k \frac{\langle v, u_i \rangle}{\langle u_i, u_i \rangle} u_i.\]
It is the unique vector $\hat{v} \in W$
which has the property that $v-\hat{v}$ is orthogonal to all $w\in W$.  Additionally, $\mathrm{Proj}_W(v)$ is 
the closest vector in $W$ to $v$.  


\endedxtext



\endedxvertical


\beginedxvertical{Projection Practice}



\beginedxproblem{Projection Practice 1}{\dpa5}


Let $W$ be the subspace of $\R^4$ spanned by the list $\{w_1; w_2; w_3\}$, where 
\[ w_1 = \left[\begin{array}{c} 1 \\ 1 \\ 2 \\ -3 
\end{array} \right], w_2 = \left[\begin{array}{c} 1 \\ -1 \\ 0 \\ 0 
\end{array} \right], w_3 = \left[\begin{array}{c} 1 \\ 1 \\ 2 \\ 2 
\end{array} \right]. \]  
Note that $\{w_1; w_2; w_3\}$ is an orthogonal list.  

Let $v = \left[\begin{array}{c} 2 \\ 1 \\ -3 \\ 4 
\end{array} \right].$  What is the closest vector to $v$ in $W$?  

\input{vectorentry.tex}

\edXabox{type="custom" cfn="VectorEntry" expect="[[0],[-1],[-1],[4]]"}


\edXsolution{ 
We seek the orthogonal projection of $v$ onto $W$.  Since we have an orthogonal basis of $W$, we can
simply apply the projection formula:

\[
\begin{array}{rcl}
\mathrm{Proj}_W(v) & = & \frac{\langle v, w_1 \rangle}{\langle w_1, w_1 \rangle} w_1 + 
\frac{\langle v, w_2 \rangle}{\langle w_2, w_2 \rangle} w_2 +
\frac{\langle v, w_3 \rangle}{\langle w_3, w_3 \rangle} w_3  \\
&  = & \frac{-15}{15} w_1 + \frac{1}{2} w_2 + \frac{5}{10}w_3 \\
&  = & \left[\begin{array}{c} 0 \\ -1 \\ -1 \\ 4 
\end{array} \right]. 
\end{array}
\]
}

\endedxproblem


\beginedxproblem{Projection Practice 2}{\dpa5}

Recall that $C[0,1]$ is the vector space of continuous functions on the interval $[0,1]$, and that
we have an inner product on that vector space given by $\langle f, g \rangle = \int_0^1 f(t)g(t) \ dt$.  


Let $W$ be the subspace of $C[0,1]$ which is the span of a single function, 
$f(t) = t$.  

Let $g(t) = t^2$.  What is the orthogonal projection of $g$ onto $W$?  

Remember to use * for multiplication.

\edXabox{type="formula" expect="0.75*t" samples="t@1:5#5" feqin="1" tolerance=".01"}\\


\edXsolution{ Again, we have an orthogonal basis of $W$, namely $\{f\}$.  
So we use the projection formula: the function that we seek is 
\[
\frac{\langle g, f \rangle}{\langle f, f \rangle} f = \frac{\int_0^1 t^3 \ dt}{\int_0^1 t^2 \ dt}t= 
\frac{1/4}{1/3}t = \frac{3}{4}t.  
\]
}

\endedxproblem



\endedxvertical


\beginedxvertical{Orthogonality as Super-Independence}



\beginedxproblem{Incomplete Basis}{\dpa2}

Suppose that $\{w_1;w_2\}$ forms a basis of $\R^2$, and let $v$ be another vector in $\R^2$.
We have drawn $v$ and $w_1$ below, but have deliberately omitted $w_2$.  



\begin{center}
\includesvg[300]{c4s2superind}   
\end{center}

We know that $v$ can be written as a linear combination 
$v = a_1w_1 + a_2w_2.$  From the given information, what are the possibilities for $a_1$?  Click all that
might apply.    


\edXabox{type="oldmultichoice" expect="$a_1$ could be positive","$a_1$ could be negative","$a_1$ could be zero" options="$a_1$ could be positive","$a_1$ could be negative","$a_1$ could be zero"}

\edXsolution{


Here is a scenario in which $a_1$ could be positive:
\begin{center}
\includesvg[300]{c4s2superindsol1}   
\end{center}


Here is a scenario in which $a_1$ could be negative:
\begin{center}
\includesvg[300]{c4s2superindsol2}   
\end{center}


And here is a scenario in which $a_1$ could be zero:
\begin{center}
\includesvg[300]{c4s2superindsol3}   
\end{center}

}

\endedxproblem



\beginedxproblem{Another Closest Vector?}{\dpa3}


Let $v = \left[ \begin{array}{c} 3 \\ 4 \\ 7 \\ 1 \end{array} \right].$  What is the vector $w\in \R^4$ that 
is closest to $v$?  

(You should not need to do any calculation!)  




\input{vectorentry2.tex}


\edXabox{type="custom" cfn="VectorEntry" expect="[[3],[4],[7],[1]]"}

\edXsolution{
Certainly $v$ itself is the only vector that is distance zero from $v$!  
}

\endedxproblem




\doedxvideo{Super-Independence}{Fz_44gpdZfU}




\beginedxproblem{Find a Coefficient}{\dpa3}


Let $\{w_1; w_2; w_3\}$ be an {\emph{orthogonal}] basis of a subspace $W$ of $\R^4$.  
Suppose that 
$w_1 = \left[ \begin{array}{c} 1 \\ -1 \\ -1 \\ 1 \end{array} \right]$.  

Suppose further that $v = \left[ \begin{array}{c} 3 \\ 4 \\ 7 \\ 1 \end{array} \right]$ is also 
an element of $W$.   

Since it is an element of $W$, we can write $v$ as a 
linear combination $v = a_1 w_1 + a_2 w_2 + a_3w_3$.  
What is $a_1$?  


\edXabox{type="numerical" expect="-1.75" feqin="1"  tolerance=".01"}


\edXsolution{
We know that $v$ is its own projection onto $W$.  Hence 
\[
a_1 w_1 + a_2 w_2 + a_3w_3 = v = \mathrm{Proj}_W(v)  = \frac{\langle v, w_1 \rangle}{\langle w_1, w_1 \rangle} w_1 + 
\frac{\langle v, w_2 \rangle}{\langle w_2, w_2 \rangle} w_2 +
\frac{\langle v, w_3 \rangle}{\langle w_3, w_3 \rangle} w_3. \]
Hence $a_1 = \frac{\langle v, w_1 \rangle}{\langle w_1, w_1 \rangle} = -\frac{7}{4}.$
}

\endedxproblem


\endedxvertical


\beginedxvertical{Gram-Schmidt Algorithm}


\doedxvideo{The Gram-Schmidt Algorithm}{Nph2ZJ9WjYs}



\beginedxproblem{Gram-Schmidt Practice 1}{\dpa3}

Let $u_1 = \left[ \begin{array}{c} 1 \\ -1 \\ -1 \\ 1 \end{array} \right]$,
$u_2 = \left[ \begin{array}{c} 1 \\ 4 \\ 3 \\ -2 \end{array} \right]$,
and $u_3 = \left[ \begin{array}{c} 0 \\ -7 \\ 0 \\ 1 \end{array} \right]$.  

If we apply the Gram-Schmidt process (as described in the video) to the basis $\{u_1; u_2; u_3\}$,
we will get $\{w_1; w_2; w_3\}$, where $w_1 = u_1$.  After the second step, we calculate
$w_2$.  What is it?  

\input{vectorentry2.tex}


\edXabox{type="custom" cfn="VectorEntry" expect="[[3],[2],[1],[0]]"}

\edXsolution{
The Gram-Schmidt process gives $w_2 = u_2 - \mathrm{Proj}_{W_1}(u_2)$, where $W_1$ is the span of 
$w_1$.  This projection is given by 
\[ \mathrm{Proj}_{W_1}(u_2) = \frac{\langle u_2, w_1 \rangle}{\langle w_1, w_1 \rangle} w_1 = \frac{-8}{4}w_1 = \left[ \begin{array}{c} -2 \\ 2 \\ 2 \\ -2 \end{array} \right],\]
so $w_2 = u_2 - \mathrm{Proj}_{W_1}(u_2) = \left[ \begin{array}{c} 3 \\ 2 \\ 1 \\ 0 \end{array} \right].$

}

\endedxproblem

\beginedxproblem{Gram-Schmidt Practice 3}{\dpa3}

Finish the algorithm to calculate $w_3$.  


\input{vectorentry2.tex}


\edXabox{type="custom" cfn="VectorEntry" expect="[[1],[-3],[3],[-1]]"}

\edXsolution{
Again, $w_3$ will be $u_3$ minus its projection onto the span of $w_1$ and $w_2$ (which we can call $W_2$). 
The projection is
\[ \mathrm{Proj}_{W_2}(u_3) = \frac{\langle u_3, w_1 \rangle}{\langle w_1, w_1 \rangle} w_1 +
\frac{\langle u_3, w_2 \rangle}{\langle w_2, w_2 \rangle} w_2 = \frac{8}{4}w_1 +  \frac{-14}{14}w_2
=\left[ \begin{array}{c} -1 \\ -4 \\ -3 \\ 2 \end{array} \right],\]
so $w_3 = u_3 -\mathrm{Proj}_{W_2}(u_3) = \left[ \begin{array}{c} 1 \\ -3 \\ 3 \\ -1 \end{array} \right].$
}

\endedxproblem




\endedxvertical






