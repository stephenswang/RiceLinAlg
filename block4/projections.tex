

\beginedxvertical{Page One}

\beginedxtext{Preliminaries}





At the end of this sequence, and after some practice, you should be able to:

\begin{itemize}
\item Recognize the properties of orthogonal projections.  
\item Be able to compute an orthogonal projection. 
\end{itemize}


For time budgeting purposes, this sequence has 3 videos totaling 23 minutes, 
plus some questions.  




\endedxtext

\endedxvertical



\beginedxvertical{Finding Closest Vectors}

\beginedxproblem{Closest Vector}{\dpa4}

Let $W$ be the subspace of $\R^3$ given by 
\[ W = \left\{ \left[  \begin{array}{c} a_1 \\ a_2 \\ 0 \end{array} \right] \ : \ a_1, a_2 \in \R \right\}.\]
(In other words, you can think of $W$ as the $xy$ plane inside $\R^3$.  

Let $v = \left[ \begin{array}{c} 3 \\ 4 \\ 7 \end{array} \right].$  What is the vector $w\in W$ that 
is closest to $v$ (i.e., the vector such that the distance from $w$ to $v$ is smallest)?  


\begin{center}
\includesvg[300]{c4s2projection}   
\end{center}


\input{vectorentry.tex}


\edXabox{type="custom" cfn="VectorEntry" expect="[[3],[4],[0]]"}

\edXsolution{
If $w = \left[ \begin{array}{c} a_1 \\ a_2 \\ 0 \end{array} \right]$, then the
distance from $v$ to $w$ is 
\[ \|w-v\| = \sqrt{(a_1-3)^2 + (a_2-4)^2 + 7^2}. \]  
At minimum, this distance is $\sqrt{49} = 7$, and it equals 7 exactly when $a_1 = 3$ and $a_2=4$.  
In other words, the distance is minimized when 
$w =  \begin{array}{c} 3 \\ 4 \\ 0  \end{array} \right].$
}

\endedxproblem



\doedxvideo{Introducing the Orthogonal Projection}{R1XIoRDaAB8}




\beginedxproblem{Exploring the formula 1}{\dpa1}

The formula given in the video is $\ds \sum_{i=1}^k \langle v, w_i\rangle w_i.$  Let's examine this
a little more closely.

For each $i$, what kind of object is $\langle v, w_i \rangle$?  

\edXabox{expect="a scalar" options="a scalar","a vector in V","a matrix","none of these"}

\edXsolution{
The inner product of two vectors must be a scalar.  
}

\endedxproblem


\beginedxproblem{Exploring the formula 2}{\dpa1}

 

\edXinline{This means that  $\ds \sum_{i=1}^k \langle v, w_i\rangle w_i$  is}
\edXabox{expect="a linear combination of" options="an inner product of","a scalar multiple of","a linear combination of","a linear transformation of","a matrix whose columns are" inline="1"} \edXabox{expect="the collection of vectors w_1...w_k" options="v","one of the vectors w_i","the collection of vectors w_1...w_k" inline="1"}

\edXinline{Therefore, it is}
\edXabox{expect="a vector in W" options="a scalar","a vector in W","a vector in V but not necessarily in W","a matrix","none of these"  inline="1"}


\edXsolution{
Each $\langle v, w_i\rangle w_i$ is a scalar times a vector $w_i$.  When we take the sum of these,
it is (by definition) a linear combination of $\{w_1; w_2; \ldots w_k\}$, and thus is a vector.  Specifically,
it is a vector in $W$, since all the $w_i$ are in $W$.      
}

\endedxproblem



\endedxvertical



\beginedxvertical{The Projection Formula Proof}


\doedxvideo{Proving the Orthogonal Projection Formula}{V5WPw2C-ixY}



\beginedxproblem{Using the Projection Formula?}{\dpa1}

Let $W$ be the span of $\{w_1; w_2\}$, where 
$w_1 = \left[ \begin{array}{c} 1 \\ 2 \\ 3 \end{array} \right]$
and 
$w_2 = \left[  \begin{array}{c} 3 \\ -3 \\ 1 \end{array} \right]$.
Then, for any $v\in \R^3$, 
\[\mathrm{Proj}_W(v) = \langle v, w_1 \rangle w_1 + \langle v, w_2 \rangle w_2.\]

\edXabox{expect="False" options="True","False"}

\edXsolution{
The formula given in the video does not work if $\{w_1; w_2\}$ is not an orthonormal basis.  
}

\endedxproblem


\endedxvertical


\beginedxvertical{Full Generality}


\doedxvideo{Final Projection Formula}{DzwlyaY4LmY}



% \beginedxproblem{Using the Projection Formula? 2}{\dpa1}

% Let $W$ be the span of $\{w_1; w_2\}$, where 
% $w_1 = \begin{array}{c} 1 \\ 2 \\ 3 \end{array} \right]$
% and 
% $w_2 = \begin{array}{c} 3 \\ -3 \\ 1 \end{array} \right]$.
% Then, for any $v\in \R^3$, the closest vector in $W$ to $v$ is given by the formula 
% \[\frac{\langle v, w_1 \rangle}{\langle w_1, w_1 \rangle} w_1 + \frac{\langle v, w_2 \rangle}{\langle w_2, w_2 \rangle} w_2.\]

% \edXabox{expect="True" options="True","False"}

% \edXsolution{
% This is the projection formula given in the video.  Since $\{w_1; w_2\}$ is an orthogonal basis of $W$,
% this 
% }

% \endedxproblem

\beginedxtext{The Orthogonal Projection}

If $V$ is a vector space with an inner product, and $W$ is a subspace of $V$ with an orthogonal basis 
$\{u_1; u_2; \ldots u_k\}$, then the {\keyb{\bf{orthogonal
projection}}} of a vector $v\in V$ onto $W$ is given by the formula
\[\mathrm{Proj}_W(v) =  \sum_{i=1}^k \frac{\langle v, u_i \rangle}{\langle u_i, u_i \rangle} u_i.\]
It is the unique vector $\hat{v} \in W$
which has the property that $v-\hat{v}$ is orthogonal to all $w\in W$.  Additionally, $\mathrm{Proj}_W(v)$ is 
the closest vector in $W$ to $v$.  


\endedxtext



\endedxvertical


\beginedxvertical{Projection Practice}



\beginedxproblem{Projection Practice 1}{\dpa1}


Let $W$ be the subspace of $\R^4$ spanned by the list $\{w_1; w_2; w_3\}$, where 
\[ w_1 = \left[\begin{array}{c} 1 \\ 1 \\ 2 \\ -3 
\end{array} \right], w_2 = \left[\begin{array}{c} 1 \\ -1 \\ 0 \\ 0 
\end{array} \right], w_3 = \left[\begin{array}{c} 1 \\ 1 \\ 2 \\ 2 
\end{array} \right]. \]  
Note that $\{w_1; w_2; w_3\}$ is an orthogonal list.  

Let $v = \left[\begin{array}{c} 2 \\ 1 \\ -3 \\ 4 
\end{array} \right].$  What is the closest vector to $v$ in $W$?  

\input{vectorentry.tex}

\edXabox{type="custom" cfn="VectorEntry" expect="[[3],[4],[0]]"}


\edXsolution{ 
}

\endedxproblem


\beginedxproblem{Projection Practice 2}{\dpa1}

Recall that $C[0,1]$ is the vector space of continuous functions on the interval $[0,1]$, and that
we have an inner product on that vector space given by $\langle f, g \rangle = \int_0^1 f(x)g(x) \ dx$.  


Let $W$ be the subspace of $C[0,1]$ which is the span of a single function 
$f(x) = x$.  

Let $g(x) = x^2$.  What is the orthogonal projection of $g$ onto $W$?  

(Don't forget to use * for multiplication.)

\edXabox{type="formula" expect="0.75*x" samples="x@1:5#5" feqin="1" tolerance=".01"}\\


\edXsolution{ 
}

\endedxproblem



\endedxvertical


\beginedxvertical{Orthogonality as Super-Independence}



\beginedxproblem{Incomplete Basis}{\dpa2}

Suppose that $\{w_1;w_2\}$ forms a basis of $\R^2$, and let $v$ be another vector in $\R^2$.
We have drawn $v$ and $w_1$ below, but have deliberately omitted $w_2$.  



\begin{center}
\includesvg[300]{c4s2superind}   
\end{center}

We know that $v$ can be written as a linear combination 
$v = a_1w_1 + a_2w_2.$  From the given information, what are the possibilities for $a_1$?  Click all that
might apply.    


\edXabox{type="oldmultichoice" expect="$a_1$ could be positive","$a_1$ could be negative","$a_1$ could be zero" options="$a_1$ could be positive","$a_1$ could be negative","$a_1$ could be zero"}

\edXsolution{


Here is a scenario in which $a_1$ could be positive:
\begin{center}
\includesvg[300]{c4s2superindsol1}   
\end{center}


Here is a scenario in which $a_1$ could be negative:
\begin{center}
\includesvg[300]{c4s2superindsol2}   
\end{center}


And here is a scenario in which $a_1$ could be zero:
\begin{center}
\includesvg[300]{c4s2superindsol3}   
\end{center}

}

\endedxproblem



\beginedxproblem{Another Closest Vector?}{\dpa2}


Let $v = \left[ \begin{array}{c} 3 \\ 4 \\ 7 \\ 1 \end{array} \right].$  What is the vector $w\in \R^4$ that 
is closest to $v$?  

(You should not need to do any projection calculation!)  




\input{vectorentry2.tex}


\edXabox{type="custom" cfn="VectorEntry" expect="[[3],[4],[7],[1]]"}

\edXsolution{
Certainly $v$ itself is the only vector that is distance zero from $v$!  
}

\endedxproblem




\doedxvideo{Super-Independence}{Fz_44gpdZfU}




\beginedxproblem{Find a Coefficient}{\dpa3}


Let $\{w_1; w_2; w_3\}$ be an {\emph{orthogonal}] basis of a subspace $W$ of $\R^4$.  
Suppose that 
$w_1 = \left[ \begin{array}{c} 1 \\ -1 \\ -1 \\ 1 \end{array} \right]$.  

Suppose further that $v = \left[ \begin{array}{c} 3 \\ 4 \\ 7 \\ 1 \end{array} \right]$ is also 
an element of $W$.   

Since it is an element of $W$, we can write $v$ as a 
linear combination $v = a_1 w_1 + a_2 w_2 + a_3w_3$.  
What is $a_1$?  


\edXabox{type="numerical" expect="-1.75" feqin="1"  tolerance=".01"}


\edXsolution{

}

\endedxproblem


\endedxvertical


\beginedxvertical{Gram-Schmidt Algorithm}


\doedxvideo{The Gram-Schmidt Algorithm}{Nph2ZJ9WjYs}



\beginedxproblem{Gram-Schmidt Practice 1}{\dpa3}

Let $u_1 = \left[ \begin{array}{c} 1 \\ -1 \\ -1 \\ 1 \end{array} \right]$,
$u_2 = \left[ \begin{array}{c} 1 \\ 4 \\ 3 \\ -2 \end{array} \right]$,
and $u_3 = \left[ \begin{array}{c} 0 \\ -7 \\ 2 \\ 3 \end{array} \right]$.  

If we apply the Gram-Schmidt process (as described in the video) to the basis $\{u_1; u_2; u_3\}$,
we will get $\{w_1; w_2; w_3\}$, where $w_1 = u_1$.  After the second step, we calculate
$w_2$.  What is it?  

\input{vectorentry2.tex}


\edXabox{type="custom" cfn="VectorEntry" expect="[[3],[2],[1],[0]]"}

\edXsolution{

}

\endedxproblem

\beginedxproblem{Gram-Schmidt Practice 3}{\dpa3}

Finish the algorithm to calculate $w_3$.  


\input{vectorentry2.tex}


\edXabox{type="custom" cfn="VectorEntry" expect="[[1],[-3],[5],[1]]"}

\edXsolution{

}

\endedxproblem




\endedxvertical






